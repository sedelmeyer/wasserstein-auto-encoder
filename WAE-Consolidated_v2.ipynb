{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Introduction to the Wasserstein Auto-encoder\n",
    "\n",
    "-------\n",
    "\n",
    "## Authors\n",
    "Joel Dapello<br>\n",
    "Michael Sedelmeyer<br>\n",
    "Wenjun Yan\n",
    "\n",
    "-------\n",
    "\n",
    "<a id=\"top\"></a>\n",
    "## Contents\n",
    "\n",
    "- [Introduction and motivation for the Wasserstein auto-encoder](#intro)\n",
    "- [Conceptual frameworks and algorithms](#concepts)\n",
    "- [Comparing results by model type](#results)\n",
    "    - [MNIST results](#mnist)\n",
    "    - [FashionMNIST results](#fmnist)\n",
    "    \n",
    "    \n",
    "- [Conclusions and further analysis](#conclusion)\n",
    "- [References](#sources)\n",
    "\n",
    "- [Appendices: PyTorch Implementation](#appendix)\n",
    "    - [Appendix A: Auto-encoder](#ae)\n",
    "    - [Appendix B: Variational auto-encoder](#vae)\n",
    "    - [Appendix C: Wasserstein auto-encoder](#wae)\n",
    "    - [Appendix C: Plotting functions](#plots)\n",
    "    \n",
    "**A list of exhibits contained in this tutorial**\n",
    "\n",
    "- [Figure 1: Conceptual comparison of auto-encoder reconstruction methods](#fig1)\n",
    "- [Figure 2: A representation of the convolutional auto-encoder model](#fig2)\n",
    "- [Figure 3: A representation of the convolutional variational auto-encoder model](#fig3)\n",
    "- [Figure 4: A representation of the convolutional Wasserstein auto-encoder model](#fig4)\n",
    "- [Figure 5: Comparison of test reconstruction loss results with MNIST and FashionMNIST](#fig5)\n",
    "- [Figure 6: Comparison of MNIST reconstructions by auto-encoder](#fig6)\n",
    "- [Figure 7: Comparison of MNIST random latent space samples reconstructed](#fig7)\n",
    "- [Figure 8: Comparison of MNIST linear interpolations by auto-encoder](#fig8)\n",
    "- [Figure 9: Comparison of FashionMNIST reconstructions by auto-encoder](#fig9)\n",
    "- [Figure 10: Comparison of FashionMNIST random latent space samples reconstructed](#fig10)\n",
    "- [Figure 11: Comparison of MNIST linear interpolations by auto-encoder](#fig11)\n",
    "- [Figure 12: Detailed plot of FashionMNIST linear interpolations by auto-encoder](#fig12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro\"></a>\n",
    "## Introduction and motivation for the Wasserstein auto-encoder\n",
    "[return to top](#top)\n",
    "\n",
    "#### Generative models and the principles of a basic auto-encoder\n",
    "Designing generative models capabale of capturing the structure of very high dimensional data is a standing problem in the field of statistical modeling. One class of models that have proved effective for this task is the auto-encoder (AE). AEs are neural network based models that assume the high dimensional data being modeled can be reduced to a lower dimensional manifold, defined on a space of latent variables. To do this, the AE defines an encoder network $Q$ which maps a high dimensional input to a low dimensional latent space $Z$, and a generator network $G$ which maps $Z$ back to the high dimensional input space. The whole system is trained end to end with stochastic gradient descent, where, in the case of the vanilla AE, the cost function is designed to minimize the distance between the training data $X$ and it's reconstruction, $\\hat{X} = G(Q(X))$. While the standard AE is quite effective at learning a low dimensional representation of the training data, it is prone to overfitting, and typically fails as a generative model. This is because with no constraint on the shape of the learned representation in latent space, it is unclear how to effectively sample from $Z$. For instance, passing randomly drawn latent codes which are far from the those that G has learned to decode often lead to the generation of nonsense.\n",
    "\n",
    "#### Introducing \"learning\" to the generative model with the variational auto-encoder\n",
    "The well-known variational auto-encoder (VAE) (Kingma & Welling, 2014) was introduced as a solution to this problem. The VAE builds on the AE frame work with a modified cost function designed to maximize the evidence lower bound between the model and target distribution. This effectively introduces a regularization penalty which pushes $Q_z=Q(Z|X=x)$ to match a specified prior distribution, $P_z$. Thus, the VAE functions as a much more powerful generative model than the standard AE, because samples drawn from the $P_z$ are in a range that the $G$ has learned to generate from. Unfortunately, while the VAE performs admirably on simple datasets such as MNIST, with more complex datasets the VAE tends to recreate blurred samples.\n",
    "\n",
    "#### Improving disentanglement in learning with the Wasserstein auto-encoder\n",
    "In 2018 with the International Conference on Learning Representations paper \"Wasserstein Auto-Encoders\", the authors Tolstikhin et. al. propose the Wasserstein auto-encoder (WAE) as a new algorithm for building a latent-variable-based generative model. This new addition to the family of regularized auto-encoders aims to minimize the optimal transport cost, $\\mathcal{D}_Z(Q_Z,P_Z)$ (Villani, 2003) formulated as the Wasserstein distance between the model distribution $Q_Z$ and the target $P_Z$ distribution. This can be thought of intuitively as the cost to transform one distribution into another, and leads to a different regularization penalty than that of the VAE. The WAE regularizer encourages the full encoded training distribution to form a continuous mixturing matching the $P_Z$ rather than individual samples as happens in the case of the VAE (see [Figure 1](#fig1)). For this reason, the WAE shares many of the properties of VAEs, while generating better quality samples due to a better disentangling of the latent space due to the optimal transport penalty.\n",
    "\n",
    "#### About this tutorial\n",
    "In this tutorial, we implement the generative adversarial network (GAN) formulation of WAE (WAEgan). The WAEgan uses the Kantorovich-Rubinstein duality (CITE), expressed as an adversarial objective on the latent space. Specifically, the WAEgan implements a discriminator network $D$ in the latent space $Z$ trying to differentiate between samples drawn from $P_Z$ and samples drawn from $Q_Z$, essentially setting $\\mathcal{D}_Z(Q_Z,P_Z)=D(Q_Z,P_Z)$, and forcing $Q$ to learn to generate latent codes that fool the discriminator $D$. In addition to implementing the WAEgan, we implement a VAE and vanilla AE as well. We choose this approach because, to better understand the WAE and its benefits, it is important to consider WAE within the context of these two preceeding and well-established algorithms. This approach provides a more intuitive understanding of the results by demonstrating side-by-side comparisons of each algorithm applied to the popular MNIST (CITE) and FashionMNIST (CITE) datasets with convolutional nueral network (CNN) implementations in PyTorch. \n",
    "\n",
    "<a id=\"fig1\"></a>\n",
    "**Figure 1:** Conceptual comparison of AE reconstruction methods (after Tolsikhin et al., 2018). All three algorithms map inputs $x \\in X$ to a latent code $z \\in Z$ and then attempt to reconstruct $\\hat{x}=G(z)$. The AE places no regularization penalty on $Z$, while the VAE and WAE use Kullback–Leibler divergence (KLD) and optimal transport cost respectively to penalize divergence of $Q_Z$ from the shape of the prior, $P_Z$. While KLD forces Q(Z|X=x) to match $P_Z$, the optimal transport cost enforces the continuous mixture $Q_z:=\\int Q(Z|X) dP_x$ to match $P_Z$.\n",
    "\n",
    "<img src=\"./images/figure 1 - reconstruction.png\" class=\"center\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"concepts\"></a>\n",
    "## Conceptual frameworks and algorithms\n",
    "[return to top](#top)\n",
    "\n",
    "To better understand WAE, it is helpful to consider the development of generative auto-encoders beginning with a simple non-stochastic AE, followed by the VAE, and then on to WAE.\n",
    "\n",
    "\n",
    "In its simplest representation, an auto-encoder consists of two networks, an encoder network and a decoder network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The auto-encoder\n",
    "<a id=\"fig2\"></a>\n",
    "**Figure 2:** Representation of convolutional auto-encoder model \n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/auto-encoder%20model.png?raw=true\" class=\"center\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"algo1\"></a>\n",
    "**Algorithm 1:** Vanilla auto-encoder pseudocode for computing a stochastic graient using the estimator (CITE)\n",
    "\n",
    "> Initialize the parameters fo the encoder $Q_{\\phi}$, decoder $G_{\\theta}$.\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $z_i$ from $Q_{\\phi}(x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $G_{\\theta}(z_i)$ for $i=1, \\dotsc , n$\n",
    " \n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n (x_i-\\tilde{z_i})^2 $$\n",
    "\n",
    "**end while**\n",
    "\n",
    "<br>\n",
    "### The variational auto-encoder\n",
    "<a id=\"fig3\"></a>\n",
    "**Figure 3:** Representation of convolutional variational auto-encoder model\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/variational%20auto-encoder%20model.png?raw=true\" class=\"center\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"algo2\"></a>\n",
    "**Algorithm 2:** Variational auto-encoder pseudocode for computing a stochastic gradient using the estimator (from Kingma & Welling, 2014)\n",
    "\n",
    "**Constraint:** Latent vector fits gaussian distribution.\n",
    "\n",
    "> Initialize the parameters fo the encoder $Q_{\\phi}$, decoder $G_{\\theta}$.\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Calculate mean $(\\mu)$ and standard deviation $(\\sigma)$ vector\n",
    "\n",
    ">> resample $\\{z_1, \\dotsc , z_n\\}$ from the Gaussian distribution $\\mathcal{N(\\mu,\\,\\sigma^{2})}$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n (x_i-G_\\theta(\\tilde{z_i}))^2 - \\int log\\frac{\\mathrm{d} Q_{\\phi}(x)}{\\mathrm{d} \\mathcal{N}} $$\n",
    "\n",
    "**end while**\n",
    "\n",
    "<br>\n",
    "### The Wasserstein auto-encoder\n",
    "<a id=\"fig4\"></a>\n",
    "**Figure 4:** Representation of convolutional Wasserstein auto-encoder model\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/wasserstein%20auto-encoder%20model.png?raw=true\" class=\"center\" width=\"1000\"/>\n",
    "\n",
    "<br>\n",
    "\n",
    "<a id=\"algo3\"></a>\n",
    "**Algorithm 3:** Wassertein auto-encoder with GAN-based penalty (WAE-GAN) pseudocode (after Tolsikhin et al., 2018)\n",
    "\n",
    "**Require:** Regularization coefficient $\\lambda > 0$.\n",
    "> Initialize the parameters fo the encoder $Q_{\\phi}$, decoder $G_{\\theta}$, and latent discriminator $D_{\\gamma}$.\n",
    "\n",
    "> **while** $(\\phi, \\theta)$ not converged **do**\n",
    "\n",
    ">> Sample $\\{x_1, \\dotsc , x_n\\}$ from the training set\n",
    "\n",
    ">> Sample $\\{z_1, \\dotsc , z_n\\}$ from the prior $P_z$\n",
    "\n",
    ">> Sample $\\tilde{z}_i$ from $Q_{\\phi}(Z\\vert x_i)$ for $i=1, \\dotsc , n$\n",
    "\n",
    ">> Update $D_{\\gamma}$ by ascending:\n",
    "$$\\frac{\\lambda}{n}\\sum_{i=1}^n log \\; D_{\\gamma}(z_i) + log (1-D_{\\gamma}(\\tilde{z}_i))$$\n",
    "\n",
    ">> Update $Q_{\\phi}$ and $G_{\\theta}$ by descending:\n",
    "$$\\frac{1}{n}\\sum_{i=1}^n c(x_i, G_{\\theta}(\\tilde{z}_i)) - \\lambda \\cdot log\\;D_{\\gamma}(\\tilde{z}_i)$$\n",
    "> **end while**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"results\"></a>\n",
    "## Comparing results by model type\n",
    "[return to top](#top)\n",
    "\n",
    "In this section we briefly review the results of each generative modeling method &mdash; the basic auto-encoder, the variational auto-encoder, and the Wasserstein auto-encoder &mdash; as we have implemented and applied them to both the MNIST and FashionMNIST datasets. While the MNIST dataset was used in the original WAE paper as a means to compare results between the VAE and WAE as they were implemented by Tolsikhin et al. (2017), we decided to also apply these models, along with the basic auto-encoder, to the FashionMNIST dataset as well. Our desire in doing so, is that this additional comparison would provide an example on a similar-enough dataset of matching dimensionality, but with a different, and more complex, latent manifold.\n",
    "\n",
    "#### Model parameters used\n",
    "Our results described below were derived from PyTorch implementations of our models parameterized as such:\n",
    "\n",
    "| Parameter   |  Value       |  Description          |\n",
    "|-------------|:------------:|:----------------------|\n",
    "| `dim_h`     |  40          | factor controlling the size of hidden layers |\n",
    "| `n_channel` |  1           | number of channels in the input data (MNIST is 1, aka greyscale) |\n",
    "| `n_z`       |  20          | number of dimensions in latent space |\n",
    "| `sigma`     |  1.0         | variance in n_z used for WAE random draws for the descriminator |\n",
    "| `lambda`    |  0.01        | hyperparameter for the weight of the discriminator loss |\n",
    "| `lr`        |  0.0002      | learning rate for the Adam optimizer |\n",
    "| `epochs`    |  50          | number of epochs completed by the model |\n",
    "| `batch_size`| 256          | batch size for stochastic gradient descent |\n",
    "\n",
    "Please refer to [the Appendices](#appendix) of this tutorial to review the fully coded implementations of each model in PyTorch.\n",
    "\n",
    "\n",
    "#### Test reconstruction loss results (MNIST and FashionMNIST)\n",
    "\n",
    "As was anticipated based upon the stated benefits of the WAE as compared to the particular weaknesses of the VAE described in earlier sections of this tutorial, we see improved reconstruction performance for WAE on both the MNIST and FashionMNIST datasets (see [Figure 5](#fig5) below). This would indicate that the WAE generated clearer image reconstructions which were better matched to the original test images (we inspect this a bit more closely in just a moment). However, also as was anticipated, the basic AE model outperformed both the VAE and WAE when measuring for test reconstruction loss. While this indicates that the AE is best suited for generating basic reconstructions that match the original test images &mdash; at least in the case of this implementation on the low dimension MNIST and FashionMNIST datasets &mdash; we expect the AE to perform more poorly when it comes to generating interpretable images when drawn from other points within its latent space. This is where the \"learning\" characteristics of the VAE and WAE algorithms should offer a clear benefit over the VAE.\n",
    "\n",
    "\n",
    "<a id=\"fig5\"></a>\n",
    "**Figure 5:** Comparison of test reconstruction loss results\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/reconstruction_loss.png?raw=true\" class=\"center\" width=\"500\"/>\n",
    "\n",
    "|      |  AE  | VAE  | WAE  |\n",
    "|-----------|:----:|:----:|:----:|\n",
    "| MNIST \t|0.0048|0.0095|0.0071|\n",
    "| FashionMNIST \t|0.0083|0.0137|0.0093|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"mnist\"></a>\n",
    "### MNIST results\n",
    "[return to top](#top)\n",
    "\n",
    "\n",
    "<a id=\"fig6\"></a>\n",
    "**Figure 6:** Comparison of MNIST reconstructions by auto-encoder\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/reconstructions_MNIST.png?raw=true\" class=\"center\" width=\"700\"/>\n",
    "\n",
    "<a id=\"fig7\"></a>\n",
    "**Figure 7:** Comparison of MNIST random latent space samples reconstructed\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/random_MNIST.png?raw=true\" class=\"center\" width=\"700\"/>\n",
    "\n",
    "<a id=\"fig8\"></a>\n",
    "**Figure 8:** Comparison of MNIST linear interpolations by auto-encoder\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/grid_interpolations_MNIST.png?raw=true\" class=\"center\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"fmnist\"></a>\n",
    "### FashionMNIST results\n",
    "[return to top](#top)\n",
    "\n",
    "Same as above for MNIST\n",
    "\n",
    "**images/tables to include:**\n",
    "1. same as above for MNIST, but probably smaller and with fewer examples if results demonstrate similar characteristics\n",
    "\n",
    "<a id=\"fig9\"></a>\n",
    "**Figure 9:** Comparison of FashionMNIST reconstructions by auto-encoder\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/reconstructions_FMNIST.png?raw=true\" class=\"center\" width=\"700\"/>\n",
    "\n",
    "<a id=\"fig10\"></a>\n",
    "**Figure 10:** Comparison of FashionMNIST random latent space samples reconstructed\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/random_FMNIST.png?raw=true\" class=\"center\" width=\"700\"/>\n",
    "\n",
    "<a id=\"fig11\"></a>\n",
    "**Figure 11:** Comparison of FashionMNIST linear interpolations by auto-encoder\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/grid_interpolations_FMNIST.png?raw=true\" class=\"center\" width=\"700\"/>\n",
    "\n",
    "<a id=\"fig12\"></a>\n",
    "**Figure 12:** Detailed plot of FashionMNIST linear interpolations by auto-encoder\n",
    "\n",
    "<img src=\"https://github.com/sedelmeyer/wasserstein-auto-encoder/blob/master/images/interpolations_FMNIST.png?raw=true\" class=\"center\" width=\"700\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## Conclusions and further analysis\n",
    "[return to top](#top)\n",
    "\n",
    "Here we summarize our conclusions given MNIST and FMNIST, but also describe other dataset we may want to run as comparison (e.g. celeb faces for representation on a low manifold surface such a faces, RNA expression data for investigation of a novel application of WAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"conclusion\"></a>\n",
    "## References\n",
    "[return to top](#top)\n",
    "\n",
    "Cite the papers, repos, datasets, and blogs we used in our analysis, as well as any other resources we want to direct our readers toward\n",
    "\n",
    "1. Kingma, D. P. & Welling, M. (2013). Auto-Encoding Variational Bayes.. CoRR, [abs/1312.6114](https://arxiv.org/abs/1312.6114). \n",
    "1. Tolstikhin, I. O., Bousquet, O., Gelly, S. & Schölkopf, B. (2017). Wasserstein Auto-Encoders.. CoRR, [abs/1711.01558](https://arxiv.org/abs/1711.01558). \n",
    "1. PyTorch (2018). Basic VAE Example.. GitHub Repository, https://github.com/pytorch/examples/tree/master/vae, accessed December 2, 2018.\n",
    "1. AE paper, or other primary source?\n",
    "1. MNIST?\n",
    "1. FashionMNIST?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"appendix\"></a>\n",
    "## Appendices: PyTorch Implementation\n",
    "[return to top](#top)\n",
    "\n",
    "The following appendices contain the PyTorch implementations for the auto-encoder ([Appendix A](#ae)), variational auto-encoder ([Appendix B](#vae)), and Wasserstein auto-encoder ([Appendix C](#wae)) used in this tutorial, as well as all associated Python code used to generate the plots ([Appendix D](#plots)) embedded in our above analysis of each auto-encoder applied to the MNIST and FashionMNIST datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "args = {}\n",
    "args['dim_h'] = 40            # factor controlling size of hidden layers\n",
    "args['n_channel'] = 1         # number of channels in the input data (MNIST is 1, aka greyscale)\n",
    "args['n_z'] = 20              # number of dimensions in latent space. \n",
    "args['sigma'] = 1.0           # variance in n_z\n",
    "args['lambda'] = 0.01         # hyper param for weight of discriminator loss\n",
    "args['lr'] = 0.0002           # learning rate for Adam optimizer\n",
    "args['epochs'] = 50           # how many epochs to run for\n",
    "args['batch_size'] = 256      # batch size for SGD\n",
    "args['save'] = False          # save weights at each epoch of training if True\n",
    "args['train'] = False         # train networks if True, else load networks from saved weights\n",
    "args['dataset'] = 'mnist'     # specify which dataset to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## load Dataset\n",
    "if args['dataset'] == 'mnist':\n",
    "    trainset = datasets.MNIST(\n",
    "        root='./MNIST/',\n",
    "        train=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    testset = datasets.MNIST(\n",
    "        root='./MNIST/',\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "elif args['dataset'] == 'fmnist':\n",
    "    trainset = datasets.FashionMNIST(\n",
    "        root='./FMNIST/',\n",
    "        train=True,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "\n",
    "    testset = datasets.FashionMNIST(\n",
    "        root='./FMNIST/',\n",
    "        train=False,\n",
    "        transform=transforms.ToTensor(),\n",
    "        download=True\n",
    "    )\n",
    "    \n",
    "train_loader = DataLoader(\n",
    "    dataset=trainset,\n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=testset,\n",
    "    batch_size=args['batch_size'],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"ae\"></a>\n",
    "### Appendix A: Auto-encoder \n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class AE_Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AE_Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters organized according to the popular DCGAN \n",
    "        # (Radford et. al., 2015) framework, excellent for image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class AE_Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(AE_Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate models, and set to train mode\n",
    "ae_encoder, ae_decoder = AE_Encoder(args), AE_Decoder(args)\n",
    "\n",
    "if args['train']:\n",
    "    # specify loss (mean squared error of image reconstruction)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # use the Adam optimizer, it's always a good choice\n",
    "    enc_optim = torch.optim.Adam(ae_encoder.parameters(), lr = args['lr'])\n",
    "    dec_optim = torch.optim.Adam(ae_decoder.parameters(), lr = args['lr'])\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.StepLR(enc_optim, step_size=30, gamma=0.5)\n",
    "    dec_scheduler = torch.optim.lr_scheduler.StepLR(dec_optim, step_size=30, gamma=0.5)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            ae_encoder.train()\n",
    "            ae_decoder.train()\n",
    "\n",
    "            ae_encoder.zero_grad()\n",
    "            ae_decoder.zero_grad()\n",
    "            batch_size = images.size()[0]\n",
    "\n",
    "            z_hat = ae_encoder(images)\n",
    "            x_hat = ae_decoder(z_hat)\n",
    "            train_recon_loss = criterion(x_hat, images)\n",
    "\n",
    "            train_recon_loss.backward()\n",
    "\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "        # Run validation set\n",
    "        ae_encoder.eval()\n",
    "        ae_decoder.eval()\n",
    "        for images, _ in tqdm(test_loader):\n",
    "            z_hat = ae_encoder(images)\n",
    "            x_hat = ae_decoder(z_hat)\n",
    "            test_recon_loss = criterion(x_hat, images)\n",
    "\n",
    "        if args['save']:\n",
    "            save_path = './save/AE_{}-epoch_{}.pth'\n",
    "            torch.save(ae_encoder.state_dict(), save_path.format('encoder', epoch))\n",
    "            torch.save(ae_decoder.state_dict(), save_path.format('decoder', epoch))\n",
    "\n",
    "        print(\"Epoch: [{}/{}], \\tTrain Reconstruction Loss: {}\\n\"\\\n",
    "              \"\\t\\t\\tTest Reconstruction Loss: {}\".format(\n",
    "            epoch + 1, \n",
    "            args['epochs'], \n",
    "            train_recon_loss.data.item(),\n",
    "            test_recon_loss.data.item()\n",
    "        ))\n",
    "else:\n",
    "    # load encoder and decoder weights from checkpoint\n",
    "    enc_checkpoint = torch.load('save/AE_encoder-best_{}.pth'.format(args['dataset']))\n",
    "    ae_encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "    dec_checkpoint = torch.load('save/AE_decoder-best_{}.pth'.format(args['dataset']))\n",
    "    ae_decoder.load_state_dict(dec_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"vae\"></a>\n",
    "### Appendix B: Variational auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class VAE_Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE_Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc1 = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "        self.fc2 = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        # return torch.normal(mu, std)\n",
    "        esp = torch.randn(*mu.size())\n",
    "        z = mu + std * esp\n",
    "        return z\n",
    "        \n",
    "    def bottleneck(self, h):\n",
    "        mu, logvar = self.fc1(h), self.fc2(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        h = h.squeeze()\n",
    "        z, mu, logvar = self.bottleneck(h)\n",
    "        \n",
    "        return z, mu, logvar\n",
    "\n",
    "class VAE_Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(VAE_Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate models, and set to train mode\n",
    "vae_encoder, vae_decoder = VAE_Encoder(args), VAE_Decoder(args)\n",
    "\n",
    "if args['train']:\n",
    "    # specify loss (mean squared error of pixel by pixel image reconstruction)\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # use the Adam optimizer, it's always a good choice\n",
    "    enc_optim = torch.optim.Adam(vae_encoder.parameters(), lr = args['lr'])\n",
    "    dec_optim = torch.optim.Adam(vae_decoder.parameters(), lr = args['lr'])\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.StepLR(enc_optim, step_size=30, gamma=0.5)\n",
    "    dec_scheduler = torch.optim.lr_scheduler.StepLR(dec_optim, step_size=30, gamma=0.5)\n",
    "\n",
    "    for epoch in range(args['epochs']):\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            vae_encoder.train()\n",
    "            vae_decoder.train()\n",
    "\n",
    "            vae_encoder.zero_grad()\n",
    "            vae_decoder.zero_grad()\n",
    "            batch_size = images.size()[0]\n",
    "\n",
    "            z_hat, mu, logvar = vae_encoder(images)\n",
    "            x_hat = vae_decoder(z_hat)\n",
    "            \n",
    "            BCE = nn.functional.binary_cross_entropy(\n",
    "                x_hat.view(-1,784), \n",
    "                images.view(-1, 784), \n",
    "                reduce=False\n",
    "            ).sum()\n",
    "            \n",
    "            KLD = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "            \n",
    "            ELBO = BCE - KLD\n",
    "            ELBO.backward()\n",
    "                        \n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "        # Run validation set\n",
    "        vae_encoder.eval()\n",
    "        vae_decoder.eval()\n",
    "        for images, _ in tqdm(test_loader):\n",
    "            z_hat, mu, logvar = vae_encoder(images)\n",
    "            x_hat = vae_decoder(z_hat)\n",
    "            test_recon_loss = criterion(x_hat, images) # maybe change to BCE?\n",
    "\n",
    "        if args['save']:\n",
    "            save_path = './save/VAE_{}-epoch_{}.pth'\n",
    "            torch.save(vae_encoder.state_dict(), save_path.format('encoder', epoch))\n",
    "            torch.save(vae_decoder.state_dict(), save_path.format('decoder', epoch))\n",
    "\n",
    "        print(\"Epoch: [{}/{}], \\tTrain Reconstruction Loss: {} \\tKLD:{}\\n\"\\\n",
    "              \"\\t\\t\\tTest Reconstruction Loss: {}\".format(\n",
    "            epoch + 1, \n",
    "            args['epochs'], \n",
    "            BCE.data.item(),\n",
    "            KLD.data.item(),\n",
    "            test_recon_loss.data.item()\n",
    "        ))\n",
    "else:\n",
    "    # load encoder and decoder weights from checkpoint\n",
    "    enc_checkpoint = torch.load('save/VAE_encoder-best_{}.pth'.format(args['dataset']))\n",
    "    vae_encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "    dec_checkpoint = torch.load('save/VAE_decoder-best_{}.pth'.format(args['dataset']))\n",
    "    vae_decoder.load_state_dict(dec_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"wae\"></a>\n",
    "### Appendix C: Wasserstein auto-encoder\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## create encoder model and decoder model\n",
    "class WAE_Encoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WAE_Encoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "        \n",
    "        # convolutional filters, work excellent with image data\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(self.n_channel, self.dim_h, 4, 2, 1, bias=False),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h, self.dim_h * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 2, self.dim_h * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(self.dim_h * 4, self.dim_h * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(self.dim_h * 8),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "        \n",
    "        # final layer is fully connected\n",
    "        self.fc = nn.Linear(self.dim_h * (2 ** 3), self.n_z)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = x.squeeze()\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "class WAE_Decoder(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(WAE_Decoder, self).__init__()\n",
    "\n",
    "        self.n_channel = args['n_channel']\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # first layer is fully connected\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 8 * 7 * 7),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        # deconvolutional filters, essentially the inverse of convolutional filters\n",
    "        self.deconv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(self.dim_h * 8, self.dim_h * 4, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 4, self.dim_h * 2, 4),\n",
    "            nn.BatchNorm2d(self.dim_h * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(self.dim_h * 2, 1, 4, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        x = x.view(-1, self.dim_h * 8, 7, 7)\n",
    "        x = self.deconv(x)\n",
    "        return x\n",
    "\n",
    "# define the descriminator\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.dim_h = args['dim_h']\n",
    "        self.n_z = args['n_z']\n",
    "\n",
    "        # main body of discriminator, returns [0,1]\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Linear(self.n_z, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, self.dim_h * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(self.dim_h * 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n",
    "    \n",
    "# control which parameters are frozen / free for optimization\n",
    "def free_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "def frozen_params(module: nn.Module):\n",
    "    for p in module.parameters():\n",
    "        p.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instantiate discriminator model, and restart encoder and decoder, for fairness. Set to train mode, etc\n",
    "wae_encoder, wae_decoder, discriminator = WAE_Encoder(args), WAE_Decoder(args), Discriminator(args)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "if args['train']:\n",
    "    enc_optim = torch.optim.Adam(wae_encoder.parameters(), lr = args['lr'])\n",
    "    dec_optim = torch.optim.Adam(wae_decoder.parameters(), lr = args['lr'])\n",
    "    dis_optim = torch.optim.Adam(discriminator.parameters(), lr = args['lr'])\n",
    "\n",
    "    enc_scheduler = torch.optim.lr_scheduler.StepLR(enc_optim, step_size=30, gamma=0.5)\n",
    "    dec_scheduler = torch.optim.lr_scheduler.StepLR(dec_optim, step_size=30, gamma=0.5)\n",
    "    dis_scheduler = torch.optim.lr_scheduler.StepLR(dis_optim, step_size=30, gamma=0.5)\n",
    "\n",
    "    # one and -one allow us to control descending / ascending gradient descent\n",
    "    one = torch.Tensor([1])\n",
    "    \n",
    "    for epoch in range(args['epochs']):\n",
    "\n",
    "        # train for one epoch -- set nets to train mode\n",
    "        wae_encoder.train()\n",
    "        wae_decoder.train()\n",
    "        discriminator.train()\n",
    "\n",
    "        for images, _ in tqdm(train_loader):\n",
    "            # zero gradients for each batch\n",
    "            wae_encoder.zero_grad()\n",
    "            wae_decoder.zero_grad()\n",
    "            discriminator.zero_grad()\n",
    "\n",
    "            # ======== Train Discriminator ======== #\n",
    "\n",
    "            # freeze auto encoder params\n",
    "            frozen_params(wae_decoder)\n",
    "            frozen_params(wae_encoder)\n",
    "\n",
    "            # free discriminator params\n",
    "            free_params(discriminator)\n",
    "\n",
    "            # run discriminator against randn draws\n",
    "            z = torch.randn(images.size()[0], args['n_z']) * args['sigma']\n",
    "            d_z = discriminator(z)\n",
    "\n",
    "            # run discriminator against encoder z's\n",
    "            z_hat = wae_encoder(images)\n",
    "            d_z_hat = discriminator(z_hat)\n",
    "\n",
    "            d_z_loss = args['lambda']*torch.log(d_z).mean()\n",
    "            d_z_hat_loss = args['lambda']*torch.log(1 - d_z_hat).mean()\n",
    "\n",
    "            # formula for ascending the descriminator -- -one reverses the direction of the gradient.\n",
    "            d_z_loss.backward(-one)\n",
    "            d_z_hat_loss.backward(-one)\n",
    "\n",
    "            dis_optim.step()\n",
    "\n",
    "            # ======== Train Generator ======== #\n",
    "\n",
    "            # flip which networks are frozen, which are not\n",
    "            free_params(wae_decoder)\n",
    "            free_params(wae_encoder)\n",
    "            frozen_params(discriminator)\n",
    "\n",
    "            batch_size = images.size()[0]\n",
    "\n",
    "            # run images\n",
    "            z_hat = wae_encoder(images)\n",
    "            x_hat = wae_decoder(z_hat)\n",
    "\n",
    "            # discriminate latents\n",
    "            z_hat2 = wae_encoder(Variable(images.data))\n",
    "            d_z_hat = discriminator(z_hat2)\n",
    "\n",
    "            # calculate reconstruction loss\n",
    "            # WAE is happy with whatever cost function, let's use BCE\n",
    "            BCE = nn.functional.binary_cross_entropy(\n",
    "                x_hat.view(-1,784), \n",
    "                images.view(-1, 784), \n",
    "                reduce=False\n",
    "            ).mean()\n",
    "            \n",
    "            # calculate discriminator loss\n",
    "            d_loss = args['lambda'] * (torch.log(d_z_hat)).mean()\n",
    "            \n",
    "            # we keep the BCE and d_loss on separate graphs to increase efficiency in pytorch\n",
    "            BCE.backward(one)\n",
    "            # -one reverse the direction of the gradient, minimizing BCE - d_loss\n",
    "            d_loss.backward(-one)\n",
    "\n",
    "            enc_optim.step()\n",
    "            dec_optim.step()\n",
    "\n",
    "        # test on test set\n",
    "        wae_encoder.eval()\n",
    "        wae_decoder.eval()\n",
    "        for images, _ in tqdm(test_loader):\n",
    "            z_hat = wae_encoder(images)\n",
    "            x_hat = wae_decoder(z_hat)\n",
    "            test_recon_loss = criterion(x_hat, images)\n",
    "\n",
    "        \n",
    "        if args['save']:\n",
    "            save_path = './save/WAEgan_{}-epoch_{}.pth'\n",
    "            torch.save(wae_encoder.state_dict(), save_path.format('encoder', epoch))\n",
    "            torch.save(wae_decoder.state_dict(), save_path.format('decoder', epoch))\n",
    "            torch.save(discriminator.state_dict(), save_path.format('discriminator', epoch))\n",
    "\n",
    "        # print stats after each epoch\n",
    "        print(\"Epoch: [{}/{}], \\tTrain Reconstruction Loss: {} d loss: {}, \\n\"\\\n",
    "              \"\\t\\t\\tTest Reconstruction Loss:{}\".format(\n",
    "            epoch + 1, \n",
    "            args['epochs'], \n",
    "            BCE.data.item(),\n",
    "            d_loss.data.item(),\n",
    "            test_recon_loss.data.item()\n",
    "        ))\n",
    "        \n",
    "else:\n",
    "    enc_checkpoint = torch.load('save/WAEgan_encoder-best_{}.pth'.format(args['dataset']))\n",
    "    wae_encoder.load_state_dict(enc_checkpoint)\n",
    "\n",
    "    dec_checkpoint = torch.load('save/WAEgan_decoder-best_{}.pth'.format(args['dataset']))\n",
    "    wae_decoder.load_state_dict(dec_checkpoint)\n",
    "    \n",
    "    dec_checkpoint = torch.load('save/WAEgan_discriminator-best_{}.pth'.format(args['dataset']))\n",
    "    discriminator.load_state_dict(dec_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"plots\"></a>\n",
    "### Appendix D: Plotting functions\n",
    "[return to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_rows(images, save=False, filename=\"./images/reconstructions.png\", \n",
    "              width=16, height=6.55, args=args, optimizer_name=\"Adam\", \n",
    "              encoders = [ae_encoder, vae_encoder, wae_encoder], \n",
    "              decoders = [ae_decoder, vae_decoder, wae_decoder], \n",
    "              seed=100, padding=True):\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    plot_images = []\n",
    "    \n",
    "    plot_images.append(images[:10].detach().numpy())\n",
    "\n",
    "    for i, _ in enumerate(encoders):\n",
    "        if i == 1:\n",
    "            z_reps = encoders[i](images)\n",
    "            plot_images.append(decoders[i](z_reps[0])[:10].detach().numpy())\n",
    "        else:\n",
    "            z_reps = encoders[i](images)\n",
    "            plot_images.append(decoders[i](z_reps)[:10].detach().numpy())\n",
    "\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(width, height))\n",
    "\n",
    "    for row, ax in enumerate(axes):\n",
    "        for i, img in enumerate(ax): \n",
    "            ax[i].imshow(plot_images[row][i,0], cmap='gray' )\n",
    "            ax[i].axis('off')\n",
    "            ax[i].set_aspect('equal')\n",
    "\n",
    "    plt.suptitle(\"Original images (row 1) compared to reconstructions \"\\\n",
    "                 \"for our AE (row 2), VAE (row 3), and WAE (row 4) implementations\", y=0.92, fontsize=14)\n",
    "\n",
    "    # add footnote with relevant model parameters\n",
    "    figtext_text = \"Parameters used: latent dimension size ({0}), hidden layer size ({1}), \"\\\n",
    "                \"epochs ({2}), batch size ({3}), optimizer ({4}), \\n learning rate ({5}), \"\\\n",
    "                \"WAE sigma ({6}), WAE discriminator loss lambda weight ({7})\".format(args[\"n_z\"], \n",
    "                                                                                     args[\"dim_h\"], \n",
    "                                                                                     args[\"epochs\"],\n",
    "                                                                                     args[\"batch_size\"], \n",
    "                                                                                     optimizer_name, \n",
    "                                                                                     args[\"lr\"], \n",
    "                                                                                     args[\"sigma\"], \n",
    "                                                                                     args[\"lambda\"])\n",
    "\n",
    "    plt.figtext(.5, 0.12, figtext_text, fontsize=14, va=\"top\", ha=\"center\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    if save:\n",
    "        if padding:\n",
    "            plt.savefig(filename)\n",
    "        else:\n",
    "            plt.savefig(filename, bbox_inches = 'tight', pad_inches = 0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def interpolate(z1, z2, steps=10):\n",
    "    z_int = torch.zeros([steps, z1.shape[0]])\n",
    "    \n",
    "    for i in range(z1.shape[0]):\n",
    "        start = z1[i].data.item()\n",
    "        end = z2[i].data.item()\n",
    "        z_int[:,i] = torch.linspace(start,end, steps=steps)\n",
    "    \n",
    "    return z_int\n",
    "\n",
    "\n",
    "def plot_interpolation(images, start_idx=0, end_idx=1, save=False, show=True,\n",
    "                       filename=\"./images/interpolations.png\", titles=True,\n",
    "                       width=16, height=6.55, args=args, optimizer_name=\"Adam\", \n",
    "                       encoders = [ae_encoder, vae_encoder, wae_encoder], \n",
    "                       decoders = [ae_decoder, vae_decoder, wae_decoder], \n",
    "                       seed=100, padding=True):  \n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    interp_steps = 9\n",
    "\n",
    "    plot_images = []\n",
    "    \n",
    "    start_pix = images[start_idx,0].numpy().copy()\n",
    "    end_pix = images[end_idx,0].numpy().copy()\n",
    "    diff = end_pix - start_pix\n",
    "    lin_step_size = diff/interp_steps\n",
    "    \n",
    "    for i, _ in enumerate(encoders):\n",
    "        if i == 1:\n",
    "            z_reps = encoders[i](images)\n",
    "            z_int = interpolate(z_reps[0][start_idx],z_reps[0][end_idx])\n",
    "            plot_images.append(decoders[i](z_int).detach().numpy())\n",
    "        else:\n",
    "            z_reps = encoders[i](images)\n",
    "            z_int = interpolate(z_reps[start_idx],z_reps[end_idx])\n",
    "            plot_images.append(decoders[i](z_int).detach().numpy())\n",
    "\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(width, height))\n",
    "\n",
    "    for i, img in enumerate(axes[0]):\n",
    "            axes[0][i].imshow(start_pix, cmap='gray')\n",
    "            axes[0][i].axis(\"off\")\n",
    "            axes[0][i].set_aspect('equal')\n",
    "            start_pix += lin_step_size\n",
    "    for row, ax in enumerate(axes[1:]):\n",
    "        for i, img in enumerate(ax): \n",
    "            ax[i].imshow(plot_images[row][i,0], cmap='gray' )\n",
    "            ax[i].axis('off')\n",
    "            ax[i].set_aspect('equal')\n",
    "\n",
    "    if titles:\n",
    "        plt.suptitle(\"Linear interpolation in pixel space (row 1) compared to latent spaces \"\\\n",
    "                     \"for our AE (row 2), VAE (row 3), and WAE (row 4)\", y=0.92, fontsize=14)\n",
    "\n",
    "    # add footnote with relevant model parameters\n",
    "        figtext_text = \"Parameters used: latent dimension size ({0}), hidden layer size ({1}), \"\\\n",
    "                    \"epochs ({2}), batch size ({3}), optimizer ({4}), \\n learning rate ({5}), \"\\\n",
    "                    \"WAE sigma ({6}), WAE discriminator loss lambda weight ({7})\".format(args[\"n_z\"], \n",
    "                                                                                         args[\"dim_h\"], \n",
    "                                                                                         args[\"epochs\"],\n",
    "                                                                                         args[\"batch_size\"], \n",
    "                                                                                         optimizer_name, \n",
    "                                                                                         args[\"lr\"], \n",
    "                                                                                         args[\"sigma\"], \n",
    "                                                                                         args[\"lambda\"])\n",
    "\n",
    "        plt.figtext(.5, 0.12, figtext_text, fontsize=14, va=\"top\", ha=\"center\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    if save:\n",
    "        if padding:\n",
    "            plt.savefig(filename)\n",
    "        else:\n",
    "            plt.savefig(filename, bbox_inches = 'tight', pad_inches = 0)\n",
    "    \n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_grid_interpolation(images, save=False, temp_path=\"./images/interpolations/\", \n",
    "                            filename=\"./images/grid_interpolations.png\", \n",
    "                            width=16, height=7, args=args, optimizer_name=\"Adam\", \n",
    "                            seed=100, padding=True):\n",
    "    imgs_list = []\n",
    "    \n",
    "    for i in range(9):\n",
    "        img_filename = \"{0}interpolations_{1}.png\".format(temp_path, i)\n",
    "        plot_interpolation(images, start_idx=i, end_idx=i+1, show=False,\n",
    "                           titles=False, save=True, filename=img_filename, \n",
    "                           seed=seed, padding=False)\n",
    "        imgs_list.append(img_filename)\n",
    "    \n",
    "    imgs = [ PIL.Image.open(i) for i in imgs_list ]\n",
    "\n",
    "    imgs_row_0 = np.hstack((np.asarray(i) for i in imgs[:3]))\n",
    "    imgs_row_1 = np.hstack((np.asarray(i) for i in imgs[3:6]))\n",
    "    imgs_row_2 = np.hstack((np.asarray(i) for i in imgs[6:]))\n",
    "    imgs_combined = np.vstack((imgs_row_0, imgs_row_1, imgs_row_2))\n",
    "\n",
    "    fig = plt.subplots(figsize=(width, height))\n",
    "\n",
    "    plt.imshow(imgs_combined)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.suptitle(\"    Linear interpolation in pixel space (row 1 each example) and latent spaces \"\\\n",
    "                 \"for our AE (row 2), VAE (row 3), and WAE (row 4)\", y=0.92, fontsize=14)\n",
    "\n",
    "    # add footnote with relevant model parameters\n",
    "    figtext_text = \"Parameters used: latent dimension size ({0}), hidden layer size ({1}), \"\\\n",
    "                \"epochs ({2}), batch size ({3}), optimizer ({4}), \\n learning rate ({5}), \"\\\n",
    "                \"WAE sigma ({6}), WAE discriminator loss lambda weight ({7})\".format(args[\"n_z\"], \n",
    "                                                                                     args[\"dim_h\"], \n",
    "                                                                                     args[\"epochs\"],\n",
    "                                                                                     args[\"batch_size\"], \n",
    "                                                                                     optimizer_name, \n",
    "                                                                                     args[\"lr\"], \n",
    "                                                                                     args[\"sigma\"], \n",
    "                                                                                     args[\"lambda\"])\n",
    "\n",
    "    plt.figtext(.5, 0.12, figtext_text, fontsize=14, va=\"top\", ha=\"center\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    if save:\n",
    "        if padding:\n",
    "            plt.savefig(filename)\n",
    "        else:\n",
    "            plt.savefig(filename, bbox_inches = 'tight', pad_inches = 0)    \n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_random(images, save=False, \n",
    "                filename=\"./images/random.png\", \n",
    "                width=16, height=6.55, args=args, optimizer_name=\"Adam\", \n",
    "                encoders = [ae_encoder, vae_encoder, wae_encoder], \n",
    "                decoders = [ae_decoder, vae_decoder, wae_decoder], \n",
    "                seed=100, padding=True):\n",
    "    \n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    \n",
    "    plot_images = []\n",
    "    \n",
    "    plot_images.append(torch.randn_like(images).detach().numpy())\n",
    "\n",
    "    for i, _ in enumerate(encoders):\n",
    "        if i == 1:\n",
    "            z_reps = encoders[i](images)\n",
    "            rand_z_reps = torch.randn_like(z_reps[0])\n",
    "            plot_images.append(decoders[i](rand_z_reps).detach().numpy())\n",
    "        else:\n",
    "            z_reps = encoders[i](images)\n",
    "            rand_z_reps = torch.randn_like(z_reps)\n",
    "            plot_images.append(decoders[i](rand_z_reps).detach().numpy())\n",
    "\n",
    "    fig, axes = plt.subplots(4, 10, figsize=(width, height))\n",
    "\n",
    "    for row, ax in enumerate(axes):\n",
    "        for i, img in enumerate(ax): \n",
    "            ax[i].imshow(plot_images[row][i,0], cmap='gray' )\n",
    "            ax[i].axis('off')\n",
    "            ax[i].set_aspect('equal')\n",
    "\n",
    "\n",
    "    plt.suptitle(\"  Random samples $N(0,1)$ of pixel space (row 1) compared to latent spaces \"\\\n",
    "                 \"for our AE (row 2), VAE (row 3), and WAE (row 4)\", y=0.92, fontsize=14)\n",
    "\n",
    "    # add footnote with relevant model parameters\n",
    "    figtext_text = \"Parameters used: latent dimension size ({0}), hidden layer size ({1}), \"\\\n",
    "                \"epochs ({2}), batch size ({3}), optimizer ({4}), \\n learning rate ({5}), \"\\\n",
    "                \"WAE sigma ({6}), WAE discriminator loss lambda weight ({7})\".format(args[\"n_z\"], \n",
    "                                                                                     args[\"dim_h\"], \n",
    "                                                                                     args[\"epochs\"],\n",
    "                                                                                     args[\"batch_size\"], \n",
    "                                                                                     optimizer_name, \n",
    "                                                                                     args[\"lr\"], \n",
    "                                                                                     args[\"sigma\"], \n",
    "                                                                                     args[\"lambda\"])\n",
    "\n",
    "    plt.figtext(.5, 0.12, figtext_text, fontsize=14, va=\"top\", ha=\"center\")\n",
    "\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "    \n",
    "    if save:\n",
    "        if padding:\n",
    "            plt.savefig(filename)\n",
    "        else:\n",
    "            plt.savefig(filename, bbox_inches = 'tight', pad_inches = 0)\n",
    "        \n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
